{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "31005_A3_TakeHomeExam.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/12729582/UTS_ML2019_ID12729582/blob/master/31005_A3_TakeHomeExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CHqd2scAC49",
        "colab_type": "text"
      },
      "source": [
        "*******************************************************************************\n",
        "SUBJECT NAME & ID:\n",
        "        **MACHINE LEARNING** | **31005**\n",
        "\n",
        "ASSESSMENT TASK 3:\n",
        "        **Take-home Exam**\n",
        "\n",
        "STUDENT NAME & ID: \n",
        "        **LADA MEAS**  | **12729582**\n",
        " \n",
        "QUESTION CHOSEN:\n",
        "        **Question #2: Ensemble Learning Algorithms**\n",
        "\n",
        "GITHUB LINK:\n",
        "        https://github.com/12729582/UTS_ML2019_ID12729582/blob/master/31005_A3_TakeHomeExam.ipynb\n",
        "*******************************************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k974GxRL_i6p",
        "colab_type": "text"
      },
      "source": [
        "***************************************************************************\n",
        "**Answers #2:** \n",
        "\n",
        "### **(1). Create diverse classifiers using ensemble methods**\n",
        "\n",
        "Nowadays, ensemble learning and multiple classifier models are being operated respectively by the people in machine learning industry. As reported by Gu & Jin (2014), ensemble method is a combination of multiple learning algorithms running multiple times to outperform the better and higher prediction than individual learning models.  Supported by many passed research papers, Zhang & Yunqian (2012) and Rodriguez, Kuncheva & Alonso (2006), claimed that classification issues could be determined by applying the combination of classifiers. \n",
        "\n",
        "According to Dietterich (2002), there are two primary approaches to constructing diverse classifiers using ensemble learning systems. The first approach is to independently produce each hypothesis that results in accuracy and diversity of the set of hypotheses. It assumes the outcomes will produce a low error rate and has higher accuracy than a single classifier model. The second approach is to design the hypotheses in a coupled way. The weighted vote of these hypotheses provides an excellent fit to the data. \n",
        "\n",
        "As theoretically stated by Ponti (2012), there are three principal motivations in order to combine the classifiers. Statistical motivation, or is known as worst case, was confirmed to be efficient in some applications. They believe that it is possible to join different types of classifiers. Representational motivation and is also called best case, a fusion of several classifiers could increase the performance of one best individual classifier. It was proved to be one of the techniques that produce the best performance compared to the best single classifier as well as reduce the number of errors. Computational motivation was to stabilize and boost the result of the best classifier.  However, it has been yet guaranteed that it will always perform better than one best individual classifier (Gu & Jin 2014).\n",
        "\n",
        "### **(2). Fuse the decision from individual classifiers**\n",
        "\n",
        "Ensemble systems designate a key for solving problems of individual classifiers by allowing each different or the identical classifier model to train on a separate data source severally. At that point, a succeeding meta-classifier can be prepared to realize which models or experts have better expectation precision, or which ones have learned which feature space. \n",
        "\n",
        "A study of utilizing ensemble-based systems for data fusion demonstrated to be effective on a majority of data fusions problems. Besides, there are some well-known ensemble-based fusion approaches, such as combining classifiers using Dempster–Shafer-based combination, genetic algorithms (Ponti 2012).\n",
        "\n",
        "Generally, there is two important technique to fuse the final decision. The first approach is integration.  All classifiers which implement in data training process, take part in the final decision, allowing competitive classifiers. The second approach is selection. Only one classifier among the classifiers that contributes to the data training process is used to produce the final decision. It presumes that classifiers are interdependent (Ponti 2012).\n",
        "\n",
        "### **(3). Establish the weights that individual classifiers contribute to the ensemble's answer**\n",
        "\n",
        "Ensemble learning algorithms take an alternate strategy from specific learning techniques, for instance, decision tree (DT), random forest (RF), k-nearest neighbor (KNN). Instead of discovering one best hypothesis to clarify the data, ensemble learning algorithms formulate a set of hypotheses and then have them vote to generate the new data point. It precisely builds a set of hypotheses, picks a set of weights, and constructs the voting classifier (Dietterich 2002). \n",
        "\n",
        "Generally, there are two steps in creating an ensemble. The first step is to train a set of base-classifier members  for a provided task. The second step is to combine these base-classifier members for the final prediction. Both, the accuracy as well as the diversity of each ensemble member are significantly produced for the overall performance of classifier ensembles (Gu & Jin 2014).\n",
        "\n",
        "### **(4). Two existing approaches to solving this problem**\n",
        "\n",
        "According to Ponti (2012), some existing methods are significantly used in the real-world machine learning industry. For example, using knowledge about the problem, randomization, \n",
        "varying methods, training data manipulation, input features manipulation and output features manipulation. \n",
        "\n",
        "This report will illustrate the processes of two conventional and basis existing approaches for constructing multiple classifier ensembles, training data manipulation (bagging & boosting), and output manipulation (ECOC). \n",
        "\n",
        ">**Training data manipulation approach**\n",
        "\n",
        ">>    Bootstrap aggregation (bagging): is a simplest but yet effective ensemble-based algorithms (Kuncheva & Whitaker 2003). It processes by getting bootstrap samples of objects, then trains a classifier on each sample. First, each classifier is drawn on a bootstrap sample from the original training dataset. Because of the similarities of the trained data and the original data, the individual classifiers in a bagging ensemble produce a high classification accuracy, likewise diversity. In terms of leading the diversity, the main influent is to proport different objects (input and validation) in the training dataset. Prediction of the final decision is made by picking the majority vote (Melville 2003). \n",
        "        \n",
        ">>    Boosting: is somewhat similar to bootstrap. It works by training the first base classifier on a randomly picked of subset of the available training dataset. It is trained to reduce the weighted. The chosen of the second classifier will be trained on a different subset of the original dataset. Picked off the third classifier will lastly be trained with instances of wrong prediction data points that produced by first and second classifiers. Finally, all three classifiers models will be combined through a three-way majority vote. And, the weight of each model is produced according to its accuracy on the weighted model set it was prepared on (Melville 2003).\n",
        "  \n",
        ">**Output manipulation approach**\n",
        "\n",
        ">>    Error-Correcting Output Coding (ECOC): is a well-known method that uses manipulation of output features. Each classifier model is created to solve a subset of multiple classes. Binary classifiers are used to build the code words. Moreover, the idea of using ECOC is to generate multiclass issues into two different class issues. In order to produce a good ECOC, a set of code words must provide an acceptable and understandable separation between rows, meaning creating the distances between two code words and make the lowest correlation between columns (Ponti 2012).\n",
        "\n",
        "\n",
        "### **(5). Advantages and disadvantages**\n",
        "\n",
        "The obvious advantage of using ensembles learning algorithms is when there is a need to deal with single model overfit. It also helps avoid the overfitting data, results worth the extra training dataset. It can further be used for classification and regression. To conclude, it plays an essential role in gaining more diversity, predictability, and accuracy are counted (Ponti 2012).\n",
        "\n",
        "The advantages of using the bagging technique are; it considers as a sufficient method in producing diverse classifiers. It suits for problems with relatively small training datasets. It can partition the large dataset into small sections. The benefit of using the boosting technique is that it is the most suitable method that performs well with binary class problem. It moreover helps reduce the number or error (bias and variance). It uses merely majority voting (Zhang & Yunqian 2012). \n",
        "\n",
        "The significant problems when using these techniques are; a vast accuracy might be conducted. It sometimes leads to data overfitting and the increase of the data size, which may also extend the waiting time during the experimentation.  Using ECOC method, will not work and perform well with a small number of classes (Zhang & Yunqian 2012)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifQCFJftBrEG",
        "colab_type": "text"
      },
      "source": [
        "***************************************************************************\n",
        "**Reference:**\n",
        "\n",
        "Dietterich, T. G. 2002, ‘Ensemble Learning’, in M.A. Arbib (ed), *The Handbook of Brain Theory and Neural Networks*, 2nd Edition, MIT Press, Massachusetts USA.\n",
        "\n",
        "Gu, S., Jin, Y. 2014, 'Generating Diverse and Accurate Classifiers Ensembles Using Multi-Objective Optimization', Department of Computing, *University of Surrey*, United Kingdom, viewed 7 October 2019, <https://core.ac.uk/download/pdf/30341639.pdf>.\n",
        "\n",
        "Kuncheva, L.I., Whitaker, C. 2003, ‘Machine Learning’, *Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy*, Vol 51, Issue 51, Pages 181-207.\n",
        "\n",
        "Melville, P. 2003, 'Creating Diverse Ensemble Classifiers', Ph D Proposal, The University of Texas at Austin, Taxas, viewed 6 October 2019, <http://www.cs.utexas.edu/~ml/papers/decorate-proposal-03.pdf>.\n",
        "\n",
        "Machine Learning - Sudeshna Sarkar 2016, *Introduction to Ensembles*, video recording, Youtube, viewed 05 October, <https://www.youtube.com/watch?v=nelJ3svz0_o>.\n",
        "\n",
        "Ponti , M. 2011, 'Combining Classifiers: From the Creation of Ensemble to the Decision Fusion', Graphic, Patterns and Image Tutorials,* Institute of Mathematical and Computer Sciences (ICMC)*, Brazil, viewed 6 October 2019, <https://www.researchgate.net/publication/261393553_Combining_Classifiers_From_the_Creation_of_Ensembles_to_the_Decision_Fusion>.\n",
        "\n",
        "RANJI RAJ 2019, *Machine Learning | Ensemble Methods*, video recording, Youtube, viewed 5 October 2019, <https://www.youtube.com/watch?v=SZbEZcdyOWU>. \n",
        "\n",
        "Ray, S. 2015, *5 easy questions on Ensemble Modelling everyone should know*, viewed 8 October 2019, <https://www.analyticsvidhya.com/blog/2015/09/questions-ensemble-modeling/>.\n",
        "\n",
        "Rocca J. 2019, *Ensemble methods: bagging, boosting, and stacking. Understanding the key concepts of ensemble learning*, Tennessee USA,  viewed 07 October 2019, <https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205>.\n",
        "\n",
        "Rodriguez, J.J., Kuncheva, L. I., Alonso, C.J. 2006, ‘IEEE Transactions on Pattern Analysis and Machine Intelligence’, *Rotation Forest: A New Classifier Ensemble Method*, Vol 28, Issue 10, Pages 1619 – 1630.\n",
        "\n",
        "Smolyakov, V. 2017, *Ensemble Learning to Improve Machine Learning Results*, viewed 8 October 2019, <https://blog.statsbot.co/ensemble-learning-d1dcd548e936>.\n",
        "\n",
        "The Semicolon 2017, *Ensemble Learning, Bootstrap, Aggregating (Bagging) and Boosting*, video recording, Youtube, viewed 5 October 2019, <https://www.youtube.com/watch?v=m-S9Hojj1as>.\n",
        "\n",
        "Zhang, C., Yunqian, M. 2012, *Ensemble Machine Learning*, Springer, MA USA.\n",
        "***************************************************************************\n"
      ]
    }
  ]
}